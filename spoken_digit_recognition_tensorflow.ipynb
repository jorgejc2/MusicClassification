{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to difficulties converting a Pytorch model to a Tensorflow model, this notebook uses the same CNN model as before, but written in Tensorflow. This allows it to be portable to an Android device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 19:44:51.275892: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 19:44:51.830271: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jorgejc2/.local/lib/python3.8/site-packages/nvidia/cublas/lib/:/usr/local/cuda-11.0/lib64\n",
      "2023-04-23 19:44:51.830323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jorgejc2/.local/lib/python3.8/site-packages/nvidia/cublas/lib/:/usr/local/cuda-11.0/lib64\n",
      "2023-04-23 19:44:51.830328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 19:44:52.466718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-23 19:44:52.471902: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jorgejc2/.local/lib/python3.8/site-packages/nvidia/cublas/lib/:/usr/local/cuda-11.0/lib64\n",
      "2023-04-23 19:44:52.472410: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as path\n",
    "import librosa\n",
    "\n",
    "import build.pybind_modules.dsp_module as cu\n",
    "import build.pybind_modules.matrix_module as myMatrix\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.math import confusion_matrix\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tflite_runtime.interpreter import Interpreter\n",
    "\n",
    "print('TensorFlow version:',tf.__version__)\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "for dev in physical_devices:\n",
    "    print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "MODEL_NAME = 'audio_mnist'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "FS = 48000\n",
    "DOWNSAMPLED_FS = 8000\n",
    "NFFT = 256\n",
    "NOVERLAP = -1\n",
    "NFILT = 40\n",
    "NUM_CEPS = 13\n",
    "NN_DATA_COLS = 28\n",
    "NN_DATA_ROWS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu6(x):\n",
    "    return tf.keras.activations.relu(x, max_value=6)\n",
    "\n",
    "def compute_accuracies(predicted_labels, dev_set, dev_labels):\n",
    "    yhats = predicted_labels\n",
    "    assert predicted_labels.dtype == int, \"Your predicted labels have type {}, but they should have type np.int (consider using .astype(int) on your output)\".format(predicted_labels.dtype)\n",
    "\n",
    "    if len(yhats) != len(dev_labels):\n",
    "        print(\"Lengths of predicted labels don't match length of actual labels\", len(yhats), len(dev_labels))\n",
    "        return 0., 0., 0., 0.\n",
    "    accuracy = np.mean(yhats == dev_labels)\n",
    "    conf_m = np.zeros((len(np.unique(dev_labels)),len(np.unique(dev_labels))))\n",
    "    for i,j in zip(dev_labels,predicted_labels):\n",
    "        conf_m[i,j] +=1\n",
    "\n",
    "    return accuracy, conf_m\n",
    "\n",
    "def export_model(saver, model, input_node_names, output_node_name):\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'out', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "\n",
    "    saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'out/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")\n",
    "\n",
    "\n",
    "class NeuralNet(tf.keras.Model):\n",
    "    def __init__(self, out_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3), activation=relu6, padding='same', kernel_initializer='he_uniform')\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=20, kernel_size=(3,3), activation=relu6, padding='same', kernel_initializer='he_uniform')\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate=0.1)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate=0.16)\n",
    "        self.dropout_3 = tf.keras.layers.Dropout(rate=0.12)\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense_1 = tf.keras.layers.Dense(units=5000, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.dense_2 = tf.keras.layers.Dense(units=1000, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.dense_3 = tf.keras.layers.Dense(units=out_size, activation='softmax', kernel_initializer='he_uniform')\n",
    "\n",
    "        # self.loss_fn = loss_fn\n",
    "        # self.optimizer = tf.keras.optimizers.SGD(learning_rate=lrate, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout_1(x, training=True)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dropout_2(x, training=True)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout_3(x, training=True)\n",
    "        x = self.dense_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len dev_set_labels: 200\n",
      "Len dev_set: 200\n",
      "Len train_set_labels: 1000\n",
      "Len train_set: 1000\n",
      "\n",
      "Trainset shape: (1000, 336)\n"
     ]
    }
   ],
   "source": [
    "# Load the data (should be trained and uploaded using the other spoken_digit_recognition notebook)\n",
    "\n",
    "dev_set_labels = np.loadtxt(\"l_dev_set_labels.csv\", delimiter=\",\", dtype=np.int32)\n",
    "train_labels = np.loadtxt(\"train_labels.csv\", delimiter=\",\", dtype=np.int32)\n",
    "train_set = np.loadtxt(\"train_set.csv\", delimiter=\",\", dtype=np.float64)\n",
    "dev_set = np.loadtxt(\"dev_set.csv\", delimiter=\",\", dtype=np.float64)\n",
    "\n",
    "# normalize the data between 0 and 1 in float64\n",
    "max_val = None\n",
    "min_val = None\n",
    "for arr in train_set:\n",
    "    for sample in arr:\n",
    "        if max_val == None or sample > max_val:\n",
    "            max_val = sample\n",
    "        if min_val == None or sample < min_val:\n",
    "            min_val = sample\n",
    "\n",
    "\n",
    "train_set -= min_val\n",
    "dev_set -= min_val\n",
    "max_val -= min_val\n",
    "\n",
    "train_set = train_set / max_val\n",
    "dev_set = dev_set / max_val\n",
    "\n",
    "print(\"Len dev_set_labels: {}\".format(len(dev_set_labels)))\n",
    "print(\"Len dev_set: {}\".format(len(dev_set_labels)))\n",
    "print(\"Len train_set_labels: {}\".format(len(train_labels)))\n",
    "print(\"Len train_set: {}\".format(len(train_set)))\n",
    "\n",
    "print(\"\\nTrainset shape: {}\".format(train_set.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping data to desired shape\n",
    "reshaped_dev_set = np.zeros((len(dev_set), NN_DATA_ROWS, NN_DATA_COLS))\n",
    "reshaped_train_set = np.zeros((len(train_set), NN_DATA_ROWS, NN_DATA_COLS))\n",
    "for i in range(len(dev_set)):\n",
    "    reshaped_dev_set[i] = np.reshape(dev_set[i], (NN_DATA_ROWS, NN_DATA_COLS))\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    reshaped_train_set[i] = np.reshape(train_set[i], (NN_DATA_ROWS, NN_DATA_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 12, 28, 1)]       0         \n",
      "                                                                 \n",
      " neural_net (NeuralNet)      (None, 10)                5817930   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv2d (Conv2D)           multiple                  100       |\n",
      "|                                                               |\n",
      "| max_pooling2d (MaxPooling2D  multiple               0         |\n",
      "| )                                                             |\n",
      "|                                                               |\n",
      "| conv2d_1 (Conv2D)         multiple                  1820      |\n",
      "|                                                               |\n",
      "| dropout (Dropout)         multiple                  0         |\n",
      "|                                                               |\n",
      "| dropout_1 (Dropout)       multiple                  0         |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 19:44:52.723406: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                                               |\n",
      "| dropout_2 (Dropout)       multiple                  0         |\n",
      "|                                                               |\n",
      "| flatten (Flatten)         multiple                  0         |\n",
      "|                                                               |\n",
      "| dense (Dense)             multiple                  805000    |\n",
      "|                                                               |\n",
      "| dense_1 (Dense)           multiple                  5001000   |\n",
      "|                                                               |\n",
      "| dense_2 (Dense)           multiple                  10010     |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "=================================================================\n",
      "Total params: 5,817,930\n",
      "Trainable params: 5,817,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# running dataset on model \n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "input_dim = 12 # num ceps - 1\n",
    "output_dim = 10 # number of genres\n",
    "weight_decay = 1e-2\n",
    "learning_rate = 1e-2\n",
    "\n",
    "input_layer = Input(shape=(NN_DATA_ROWS, NN_DATA_COLS, 1))\n",
    "x = NeuralNet(output_dim)(input_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# printing a view of the model\n",
    "print(model.summary(expand_nested=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 2.3296 - acc: 0.1350 - val_loss: 2.2981 - val_acc: 0.1250\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 2.2513 - acc: 0.1420 - val_loss: 2.2046 - val_acc: 0.1700\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 2.1503 - acc: 0.2370 - val_loss: 2.1263 - val_acc: 0.2250\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 2.0630 - acc: 0.3100 - val_loss: 2.0068 - val_acc: 0.3650\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 2.0277 - acc: 0.3010 - val_loss: 1.9806 - val_acc: 0.3200\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 1.9099 - acc: 0.4200 - val_loss: 1.8850 - val_acc: 0.4500\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 1.8555 - acc: 0.4340 - val_loss: 1.8272 - val_acc: 0.4250\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 1.7416 - acc: 0.4870 - val_loss: 1.6593 - val_acc: 0.5200\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 1.6531 - acc: 0.5480 - val_loss: 1.5745 - val_acc: 0.6500\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 1.5453 - acc: 0.6010 - val_loss: 1.4922 - val_acc: 0.5500\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 1.4482 - acc: 0.6060 - val_loss: 1.4260 - val_acc: 0.5800\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 1.3303 - acc: 0.6820 - val_loss: 1.3149 - val_acc: 0.6450\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 1.2234 - acc: 0.7030 - val_loss: 1.2310 - val_acc: 0.6900\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 1.1461 - acc: 0.7100 - val_loss: 1.0728 - val_acc: 0.7400\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 1.0453 - acc: 0.7500 - val_loss: 1.0014 - val_acc: 0.7150\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.9489 - acc: 0.7880 - val_loss: 0.9255 - val_acc: 0.7850\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.8678 - acc: 0.8040 - val_loss: 0.8606 - val_acc: 0.7900\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.8080 - acc: 0.8200 - val_loss: 0.8282 - val_acc: 0.8150\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.7668 - acc: 0.8260 - val_loss: 0.7510 - val_acc: 0.8250\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.6603 - acc: 0.8450 - val_loss: 0.6969 - val_acc: 0.8150\n"
     ]
    }
   ],
   "source": [
    "# training the model now\n",
    "# model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, weight_decay=weight_decay), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=\"acc\")\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, weight_decay=weight_decay), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"acc\")\n",
    "history = model.fit(x=reshaped_train_set, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(reshaped_dev_set, dev_set_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# confusion = confusion_matrix(labels=dev_set_labels, predictions=, num_classes=num_classes)\n",
    "# print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 prections by (value, label)\n",
      "[(0.29388922, 7), (0.2321858, 8), (0.18663992, 2)]\n",
      "Actual label was: 6\n",
      "\n",
      "Top 3 prections by (value, label)\n",
      "[(0.8033189, 5), (0.12497926, 7), (0.034471214, 4)]\n",
      "Actual label was: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing a prediction on the model\n",
    "dev_set_1_idx = 20\n",
    "dev_set_2_idx = 180\n",
    "single_test = tf.convert_to_tensor(np.array([reshaped_dev_set[dev_set_1_idx], reshaped_dev_set[dev_set_2_idx]]))\n",
    "predictions = model(single_test, training=False).numpy()\n",
    "\n",
    "print(\"Top 3 prections by (value, label)\")\n",
    "print(sorted(zip(predictions[0], [i for i in range(10)]), reverse=True)[:3])\n",
    "print(\"Actual label was: {}\\n\".format(dev_set_labels[dev_set_1_idx]))\n",
    "\n",
    "print(\"Top 3 prections by (value, label)\")\n",
    "print(sorted(zip(predictions[1], [i for i in range(10)]), reverse=True)[:3])\n",
    "print(\"Actual label was: {}\\n\".format(dev_set_labels[dev_set_2_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayFIR(filt):\n",
    "    coef_str = \"{\" \n",
    "    for val in filt: \n",
    "        coef_str += str(val) + \", \" \n",
    "    coef_str = coef_str[:-2] \n",
    "    coef_str += \"};\" \n",
    "    print(\"FIR a Coefficients\")\n",
    "    print(coef_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIR a Coefficients\n",
      "{0.37957937228225425, 0.3714986117842557, 0.3756342901368514, 0.36749962020527693, 0.39516165384576263, 0.4006051142599922, 0.3827173966199189, 0.3852082101473901, 0.39247934066041756, 0.37916899309645696, 0.31506156766613364, 0.3308236359151576, 0.2981971620403313, 0.2737523694189852, 0.28268743288092174, 0.3152678413998494, 0.32207365229684426, 0.3467714748818549, 0.3602618748521197, 0.3784991428750813, 0.40540582797327557, 0.36648110179403093, 0.3696725785255151, 0.35372701412876756, 0.3747523309532439, 0.41644297670673497, 0.36024640990222756, 0.377333245085688};\n"
     ]
    }
   ],
   "source": [
    "displayFIR(reshaped_dev_set[dev_set_1_idx][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, conv2d_1_layer_call_fn while saving (showing 5 of 21). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp_x3rylq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp_x3rylq/assets\n",
      "2023-04-23 19:45:10.381848: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-04-23 19:45:10.381872: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-04-23 19:45:10.382273: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpp_x3rylq\n",
      "2023-04-23 19:45:10.383198: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-04-23 19:45:10.383211: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpp_x3rylq\n",
      "2023-04-23 19:45:10.386094: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-04-23 19:45:10.386798: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-04-23 19:45:10.417347: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpp_x3rylq\n",
      "2023-04-23 19:45:10.428685: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 46413 microseconds.\n",
      "2023-04-23 19:45:10.447772: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('my_model')\n",
    "\n",
    "# Convert the model to the TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open('my_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 prections by (value, label)\n",
      "[(0.36168578, 6), (0.23412475, 7), (0.17962642, 5)]\n",
      "Actual label was: 6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'my_model.tflite'\n",
    "interpreter = Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "single_test = np.reshape(np.array(reshaped_dev_set[dev_set_1_idx], dtype=np.float32), (1, NN_DATA_ROWS, NN_DATA_COLS, 1))\n",
    "interpreter.set_tensor(input_details[0]['index'], single_test)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "print(\"Top 3 prections by (value, label)\")\n",
    "print(sorted(zip(output_data, [i for i in range(10)]), reverse=True)[:3])\n",
    "print(\"Actual label was: {}\\n\".format(dev_set_labels[dev_set_1_idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
