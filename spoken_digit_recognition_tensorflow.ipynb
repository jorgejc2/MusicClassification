{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to difficulties converting a Pytorch model to a Tensorflow model, this notebook uses the same CNN model as before, but written in Tensorflow. This allows it to be portable to an Android device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "import build.pybind_modules.dsp_module as cu\n",
    "import build.pybind_modules.matrix_module as myMatrix\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.math import confusion_matrix\n",
    "\n",
    "\n",
    "print('TensorFlow version:',tf.__version__)\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "for dev in physical_devices:\n",
    "    print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "MODEL_NAME = 'audio_mnist'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "FS = 48000\n",
    "DOWNSAMPLED_FS = 8000\n",
    "NFFT = 256\n",
    "NOVERLAP = -1\n",
    "NFILT = 40\n",
    "NUM_CEPS = 13\n",
    "NN_DATA_COLS = 48\n",
    "NN_DATA_ROWS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu6(x):\n",
    "    return tf.keras.activations.relu(x, max_value=6)\n",
    "\n",
    "def compute_accuracies(predicted_labels, dev_set, dev_labels):\n",
    "    yhats = predicted_labels\n",
    "    assert predicted_labels.dtype == int, \"Your predicted labels have type {}, but they should have type np.int (consider using .astype(int) on your output)\".format(predicted_labels.dtype)\n",
    "\n",
    "    if len(yhats) != len(dev_labels):\n",
    "        print(\"Lengths of predicted labels don't match length of actual labels\", len(yhats), len(dev_labels))\n",
    "        return 0., 0., 0., 0.\n",
    "    accuracy = np.mean(yhats == dev_labels)\n",
    "    conf_m = np.zeros((len(np.unique(dev_labels)),len(np.unique(dev_labels))))\n",
    "    for i,j in zip(dev_labels,predicted_labels):\n",
    "        conf_m[i,j] +=1\n",
    "\n",
    "    return accuracy, conf_m\n",
    "\n",
    "\n",
    "class NeuralNet(tf.keras.Model):\n",
    "    def __init__(self, out_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3), activation=relu6, padding='same', kernel_initializer='he_uniform')\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=20, kernel_size=(3,3), activation=relu6, padding='same', kernel_initializer='he_uniform')\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate=0.1)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate=0.16)\n",
    "        self.dropout_3 = tf.keras.layers.Dropout(rate=0.12)\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense_1 = tf.keras.layers.Dense(units=5000, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.dense_2 = tf.keras.layers.Dense(units=1000, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.dense_3 = tf.keras.layers.Dense(units=out_size, kernel_initializer='he_uniform')\n",
    "\n",
    "        # self.loss_fn = loss_fn\n",
    "        # self.optimizer = tf.keras.optimizers.SGD(learning_rate=lrate, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout_1(x, training=True)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dropout_2(x, training=True)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout_3(x, training=True)\n",
    "        x = self.dense_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len dev_set_labels: 200\n",
      "Len dev_set: 200\n",
      "Len train_set_labels: 1000\n",
      "Len train_set: 1000\n",
      "\n",
      "Trainset shape: (1000, 576)\n"
     ]
    }
   ],
   "source": [
    "# Load the data (should be trained and uploaded using the other spoken_digit_recognition notebook)\n",
    "\n",
    "dev_set_labels = np.loadtxt(\"l_dev_set_labels.csv\", delimiter=\",\", dtype=np.int32)\n",
    "train_labels = np.loadtxt(\"train_labels.csv\", delimiter=\",\", dtype=np.int32)\n",
    "train_set = np.loadtxt(\"train_set.csv\", delimiter=\",\", dtype=np.float64)\n",
    "dev_set = np.loadtxt(\"dev_set.csv\", delimiter=\",\", dtype=np.float64)\n",
    "\n",
    "print(\"Len dev_set_labels: {}\".format(len(dev_set_labels)))\n",
    "print(\"Len dev_set: {}\".format(len(dev_set_labels)))\n",
    "print(\"Len train_set_labels: {}\".format(len(train_labels)))\n",
    "print(\"Len train_set: {}\".format(len(train_set)))\n",
    "\n",
    "print(\"\\nTrainset shape: {}\".format(train_set.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping data to desired shape\n",
    "reshaped_dev_set = np.zeros((len(dev_set), NN_DATA_ROWS, NN_DATA_COLS))\n",
    "reshaped_train_set = np.zeros((len(train_set), NN_DATA_ROWS, NN_DATA_COLS))\n",
    "for i in range(len(dev_set)):\n",
    "    reshaped_dev_set[i] = np.reshape(dev_set[i], (NN_DATA_ROWS, NN_DATA_COLS))\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    reshaped_train_set[i] = np.reshape(train_set[i], (NN_DATA_ROWS, NN_DATA_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 12, 48, 1)]       0         \n",
      "                                                                 \n",
      " neural_net_7 (NeuralNet)    (None, 10)                6217930   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| conv2d_14 (Conv2D)        multiple                  100       |\n",
      "|                                                               |\n",
      "| max_pooling2d_7 (MaxPooling  multiple               0         |\n",
      "| 2D)                                                           |\n",
      "|                                                               |\n",
      "| conv2d_15 (Conv2D)        multiple                  1820      |\n",
      "|                                                               |\n",
      "| dropout_21 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| dropout_22 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| dropout_23 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| flatten_7 (Flatten)       multiple                  0         |\n",
      "|                                                               |\n",
      "| dense_21 (Dense)          multiple                  1205000   |\n",
      "|                                                               |\n",
      "| dense_22 (Dense)          multiple                  5001000   |\n",
      "|                                                               |\n",
      "| dense_23 (Dense)          multiple                  10010     |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "=================================================================\n",
      "Total params: 6,217,930\n",
      "Trainable params: 6,217,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# running dataset on model \n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "input_dim = 12 # num ceps - 1\n",
    "output_dim = 10 # number of genres\n",
    "weight_decay = 1e-2\n",
    "learning_rate = 1e-2\n",
    "\n",
    "input_layer = Input(shape=(NN_DATA_ROWS, NN_DATA_COLS, 1))\n",
    "x = NeuralNet(output_dim)(input_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# printing a view of the model\n",
    "print(model.summary(expand_nested=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 20ms/step - loss: 22.9464 - acc: 0.0990 - val_loss: 2.3208 - val_acc: 0.1150\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.3228 - acc: 0.1070 - val_loss: 2.3108 - val_acc: 0.0900\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.3119 - acc: 0.1050 - val_loss: 2.2835 - val_acc: 0.1250\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.3122 - acc: 0.1080 - val_loss: 2.3139 - val_acc: 0.1200\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.3077 - acc: 0.1090 - val_loss: 2.2958 - val_acc: 0.1150\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 2.3037 - acc: 0.1250 - val_loss: 2.2738 - val_acc: 0.1400\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 2.2803 - acc: 0.1530 - val_loss: 2.2657 - val_acc: 0.1250\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 2.2280 - acc: 0.1600 - val_loss: 2.2035 - val_acc: 0.2150\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 2.2058 - acc: 0.1890 - val_loss: 2.1738 - val_acc: 0.2400\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.1209 - acc: 0.2290 - val_loss: 2.0081 - val_acc: 0.2250\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 2.0492 - acc: 0.2770 - val_loss: 2.0042 - val_acc: 0.3050\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.8240 - acc: 0.3800 - val_loss: 2.0392 - val_acc: 0.2400\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.7456 - acc: 0.3860 - val_loss: 1.6791 - val_acc: 0.4100\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.5600 - acc: 0.4460 - val_loss: 1.5874 - val_acc: 0.4050\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.2726 - acc: 0.5540 - val_loss: 1.2048 - val_acc: 0.5600\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.2685 - acc: 0.5690 - val_loss: 1.8739 - val_acc: 0.4050\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1218 - acc: 0.6160 - val_loss: 1.0126 - val_acc: 0.6150\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.8569 - acc: 0.6980 - val_loss: 1.3901 - val_acc: 0.4850\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.9048 - acc: 0.6800 - val_loss: 0.9914 - val_acc: 0.6650\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.7604 - acc: 0.7210 - val_loss: 0.8013 - val_acc: 0.6700\n"
     ]
    }
   ],
   "source": [
    "# training the model now\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, weight_decay=weight_decay), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=\"acc\")\n",
    "history = model.fit(x=reshaped_train_set, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(reshaped_dev_set, dev_set_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# confusion = confusion_matrix(labels=dev_set_labels, predictions=, num_classes=num_classes)\n",
    "# print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 prections by (value, label)\n",
      "[(6.1345177, 8), (2.4934447, 6), (2.345057, 2)]\n",
      "Actual label was: 8\n",
      "\n",
      "Top 3 prections by (value, label)\n",
      "[(3.8146093, 9), (3.5829391, 1), (2.2780287, 5)]\n",
      "Actual label was: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing a prediction on the model\n",
    "dev_set_1_idx = 20\n",
    "dev_set_2_idx = 180\n",
    "single_test = tf.convert_to_tensor(np.array([reshaped_dev_set[dev_set_1_idx], reshaped_dev_set[dev_set_2_idx]]))\n",
    "predictions = model(single_test, training=False).numpy()\n",
    "\n",
    "print(\"Top 3 prections by (value, label)\")\n",
    "print(sorted(zip(predictions[0], [i for i in range(10)]), reverse=True)[:3])\n",
    "print(\"Actual label was: {}\\n\".format(dev_set_labels[dev_set_1_idx]))\n",
    "\n",
    "print(\"Top 3 prections by (value, label)\")\n",
    "print(sorted(zip(predictions[1], [i for i in range(10)]), reverse=True)[:3])\n",
    "print(\"Actual label was: {}\\n\".format(dev_set_labels[dev_set_2_idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
