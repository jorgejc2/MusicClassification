{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to difficulties converting a Pytorch model to a Tensorflow model, this notebook uses the same CNN model as before, but written in Tensorflow. This allows it to be portable to an Android device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as path\n",
    "import librosa\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import build.pybind_modules.dsp_module as cu\n",
    "import build.pybind_modules.matrix_module as myMatrix\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.math import confusion_matrix\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tflite_runtime.interpreter import Interpreter\n",
    "\n",
    "print('TensorFlow version:',tf.__version__)\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "for dev in physical_devices:\n",
    "    print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "MODEL_NAME = 'audio_mnist'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Parameters used on tablet \n",
    "VOICED_THRESHOLD = 20000000\n",
    "FRAME_SETBACK = 2\n",
    "FS = 48000\n",
    "DOWN_SAMPLED_FS = 8000\n",
    "NFFT = 256\n",
    "NOVERLAP = -1\n",
    "NFILT = 40\n",
    "NUM_CEPS = 13\n",
    "NN_DATA_COLS = 28\n",
    "NN_DATA_ROWS = 12\n",
    "PREEMPHASIS_B = 0.97\n",
    "PIXEL_WIDTH = 400\n",
    "PIXEL_HEIGHT = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu6(x):\n",
    "    return tf.keras.activations.relu(x, max_value=6)\n",
    "\n",
    "\n",
    "class NeuralNet(tf.keras.Model):\n",
    "    def __init__(self, out_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.rescaling = tf.keras.layers.Rescaling(1./255, input_shape=(300, 400, 3))\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3), activation=relu6, padding='same', kernel_initializer='he_uniform')\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=20, kernel_size=(3,3), activation=relu6, padding='same', kernel_initializer='he_uniform')\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate=0.1)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate=0.16)\n",
    "        self.dropout_3 = tf.keras.layers.Dropout(rate=0.12)\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense_1 = tf.keras.layers.Dense(units=5000, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.dense_2 = tf.keras.layers.Dense(units=1000, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.dense_3 = tf.keras.layers.Dense(units=out_size, activation='softmax', kernel_initializer='he_uniform')\n",
    "\n",
    "        # self.loss_fn = loss_fn\n",
    "        # self.optimizer = tf.keras.optimizers.SGD(learning_rate=lrate, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.rescaling(x)\n",
    "        # print(\"Done rescaling\")\n",
    "        x = self.conv1(x)\n",
    "        # print(\"Done conv1\")\n",
    "        x = self.maxpool(x)\n",
    "        # print(\"Done maxpool1\")\n",
    "        x = self.conv2(x)\n",
    "        # print(\"Done conv2\")\n",
    "        x = self.maxpool(x)\n",
    "        # print(\"Done maxpool2\")\n",
    "        x = self.dropout_1(x, training=True)\n",
    "        # print(\"Done droput1\")\n",
    "        x = self.flatten(x)\n",
    "        # print(\"Done flatten\")\n",
    "        x = self.dense_1(x)\n",
    "        # print(\"Done dense 1\")\n",
    "        x = self.dropout_2(x, training=True)\n",
    "        # print(\"Done dropout 2\")\n",
    "        x = self.dense_2(x)\n",
    "        # print(\"Done dense 3\")\n",
    "        x = self.dropout_3(x, training=True)\n",
    "        # print(\"Done dropout 3\")\n",
    "        x = self.dense_3(x)\n",
    "        # print(\"Done dense 3\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 400, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=400x300 at 0x7F616C0645B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x33\n",
      "0x63\n",
      "0x8d\n",
      "0xff\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=400x300 at 0x7F618C5FDA30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the data set\n",
    "\n",
    "path = \"MFCC_Images/0/mfcc_images/Mon May 01 14:35:23 CDT 2023_mfcc.png\"\n",
    "test_image = Image.open(path)\n",
    "image_data = np.array(test_image)\n",
    "print(image_data.shape)\n",
    "display(test_image)\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"{}\".format(hex(image_data[10][20][i])))\n",
    "\n",
    "\"\"\" \n",
    "Noticed that the fourth channel is alpha which is always xFF.\n",
    "Now I will try to figure out the RGB order by mutating the image.\n",
    "\"\"\"\n",
    "for i in range(image_data.shape[0]):\n",
    "    for j in range(image_data.shape[1]):\n",
    "        pixel = image_data[i,j]\n",
    "        pixel[2] = 0\n",
    "\"\"\"\n",
    "From experimentation:\n",
    "    channel 0 -- r\n",
    "    channel 1 -- g\n",
    "    channel 2 -- b\n",
    "    channel 3 -- alpha (always 0xFF) (also must be the most signicant byte)\n",
    "\n",
    "This is good because it verifies that the breakdown is the same as the images on the Android tablet. \n",
    "\"\"\"\n",
    "# now figure out which channel is green\n",
    "mutated_image = Image.fromarray(image_data)\n",
    "display(mutated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shuffle shape: (500, 300, 400, 4)\n",
      "after shuffle shape: (500, 300, 400, 4)\n",
      "len train set: 500\n",
      "len train label set: 500\n",
      "len dev set: 100\n",
      "len dev label set: 100\n",
      "[3. 6. 6. 7. 0. 3. 9. 4. 2. 6. 9. 9. 3. 9. 2. 4. 8. 2. 1. 6. 7. 8. 5. 7.\n",
      " 1. 5. 4. 5. 8. 0. 3. 4. 2. 7. 7. 9. 9. 9. 6. 2. 2. 8. 2. 3. 6. 6. 2. 3.\n",
      " 6. 1. 2. 7. 0. 5. 4. 3. 9. 3. 9. 0. 4. 0. 1. 2. 3. 4. 4. 0. 7. 2. 9. 1.\n",
      " 7. 0. 5. 4. 8. 0. 4. 6. 5. 2. 3. 6. 9. 2. 7. 4. 8. 8. 8. 9. 2. 1. 1. 8.\n",
      " 5. 1. 1. 8. 3. 4. 6. 6. 8. 0. 9. 6. 7. 1. 6. 5. 2. 1. 2. 3. 8. 4. 0. 8.\n",
      " 5. 4. 7. 0. 1. 9. 8. 3. 9. 3. 4. 1. 1. 5. 0. 8. 8. 7. 5. 2. 5. 2. 9. 2.\n",
      " 5. 6. 3. 6. 6. 9. 5. 4. 5. 9. 7. 1. 3. 3. 7. 1. 0. 1. 3. 2. 2. 5. 8. 4.\n",
      " 3. 9. 1. 4. 2. 5. 9. 5. 6. 4. 4. 1. 8. 6. 4. 4. 0. 7. 7. 9. 5. 3. 5. 6.\n",
      " 3. 2. 4. 0. 8. 6. 6. 9. 4. 7. 2. 0. 1. 9. 9. 9. 0. 0. 1. 9. 3. 2. 5. 0.\n",
      " 8. 7. 7. 8. 8. 6. 3. 2. 1. 0. 0. 7. 0. 7. 5. 8. 9. 7. 0. 6. 3. 7. 9. 2.\n",
      " 7. 1. 3. 1. 1. 5. 3. 6. 8. 2. 9. 7. 4. 3. 9. 5. 6. 6. 2. 4. 3. 9. 3. 3.\n",
      " 4. 2. 0. 9. 5. 5. 1. 8. 4. 6. 1. 1. 0. 2. 0. 9. 6. 5. 8. 7. 4. 2. 6. 7.\n",
      " 6. 1. 1. 6. 9. 2. 4. 0. 0. 2. 2. 7. 0. 8. 6. 0. 0. 4. 9. 9. 8. 7. 0. 9.\n",
      " 1. 2. 9. 5. 1. 7. 0. 2. 2. 2. 6. 7. 7. 1. 4. 5. 6. 8. 0. 6. 2. 6. 6. 0.\n",
      " 2. 2. 4. 8. 7. 5. 1. 8. 1. 3. 2. 4. 5. 8. 5. 8. 2. 8. 4. 0. 1. 8. 8. 7.\n",
      " 6. 5. 3. 5. 6. 2. 1. 9. 3. 5. 4. 3. 9. 5. 5. 1. 3. 5. 4. 5. 4. 7. 3. 6.\n",
      " 0. 2. 9. 3. 3. 4. 4. 4. 0. 7. 4. 7. 3. 6. 8. 0. 7. 5. 1. 4. 1. 1. 8. 7.\n",
      " 7. 0. 7. 8. 8. 4. 7. 6. 7. 2. 1. 1. 0. 6. 7. 8. 7. 2. 1. 2. 5. 7. 5. 3.\n",
      " 8. 4. 8. 5. 0. 9. 5. 8. 6. 1. 3. 3. 6. 9. 4. 0. 3. 9. 1. 3. 0. 3. 7. 1.\n",
      " 9. 0. 0. 1. 4. 5. 2. 6. 3. 7. 4. 7. 5. 4. 9. 8. 7. 9. 5. 9. 8. 0. 8. 5.\n",
      " 6. 9. 6. 1. 0. 1. 8. 3. 5. 3. 9. 0. 6. 8. 3. 8. 5. 0. 4. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "train_files_per_label = 50\n",
    "dev_files_per_label = 60 - train_files_per_label\n",
    "dev_set = np.zeros((dev_files_per_label*10, 300, 400, 3), dtype=np.int8)\n",
    "dev_set_labels = np.zeros(dev_files_per_label * 10)\n",
    "raw_train_set = np.zeros((train_files_per_label*10, 300, 400, 4), dtype=np.int8)\n",
    "train_set_labels = np.zeros(train_files_per_label * 10)\n",
    "\n",
    "path = os.getcwd() + \"/MFCC_Images\"\n",
    "labeled_directories = os.listdir(path)\n",
    "\n",
    "for i in range(len(labeled_directories)):\n",
    "    label = labeled_directories[i]\n",
    "    curr_label = int(label)\n",
    "    # print(curr_label)\n",
    "\n",
    "    image_directory_path = f\"{path}/{label}/mfcc_images\"\n",
    "    image_directory = os.listdir(image_directory_path)\n",
    "    \n",
    "    for j in range(len(image_directory)):\n",
    "        image = image_directory[j]\n",
    "        image_path = f\"{image_directory_path}/{image}\"\n",
    "        curr_image = Image.open(image_path)\n",
    "        if j < train_files_per_label:\n",
    "            raw_train_set[(i*train_files_per_label) + j,:,:,:3] = np.array(curr_image)[:,:,:3]\n",
    "            raw_train_set[(i*train_files_per_label) + j,:,:,3] = curr_label\n",
    "        else:\n",
    "            dev_set[(i*dev_files_per_label) + (j - train_files_per_label)] = np.array(curr_image)[:,:,:3]\n",
    "            dev_set_labels[(i*dev_files_per_label) + (j - train_files_per_label)] = curr_label\n",
    "\n",
    "# now randomize the training set\n",
    "# note that this only shuffles along axis=0 which is what we want\n",
    "print(\"before shuffle shape: {}\".format(raw_train_set.shape))\n",
    "np.random.shuffle(raw_train_set)\n",
    "print(\"after shuffle shape: {}\".format(raw_train_set.shape))\n",
    "train_set_label_idx = 0\n",
    "for i in range(raw_train_set.shape[0]):\n",
    "    train_set_labels[train_set_label_idx] = raw_train_set[i,0,0,3]\n",
    "    train_set_label_idx += 1\n",
    "train_set = np.zeros((train_files_per_label*10, 300, 400, 3), dtype=np.int8)\n",
    "train_set[:,:,:,:] = raw_train_set[:,:,:,:3]\n",
    "\n",
    "print(\"len train set: {}\".format(len(train_set)))\n",
    "print(\"len train label set: {}\".format(len(train_set_labels)))\n",
    "print(\"len dev set: {}\".format(len(dev_set)))\n",
    "print(\"len dev label set: {}\".format(len(dev_set_labels)))\n",
    "\n",
    "print(train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 300, 400, 3)]     0         \n",
      "                                                                 \n",
      " neural_net_9 (NeuralNet)    (None, 10)                158018110 \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rescaling_2 (Rescaling)   multiple                  0         |\n",
      "|                                                               |\n",
      "| conv2d_18 (Conv2D)        multiple                  280       |\n",
      "|                                                               |\n",
      "| max_pooling2d_9 (MaxPooling  multiple               0         |\n",
      "| 2D)                                                           |\n",
      "|                                                               |\n",
      "| conv2d_19 (Conv2D)        multiple                  1820      |\n",
      "|                                                               |\n",
      "| dropout_27 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| dropout_28 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| dropout_29 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| flatten_9 (Flatten)       multiple                  0         |\n",
      "|                                                               |\n",
      "| dense_27 (Dense)          multiple                  153005000 |\n",
      "|                                                               |\n",
      "| dense_28 (Dense)          multiple                  5001000   |\n",
      "|                                                               |\n",
      "| dense_29 (Dense)          multiple                  10010     |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "=================================================================\n",
      "Total params: 158,018,110\n",
      "Trainable params: 158,018,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# running dataset on model \n",
    "input_dim = 12 # num ceps - 1\n",
    "output_dim = 10 # number of genres\n",
    "weight_decay = 1e-2\n",
    "learning_rate = 1e-2\n",
    "\n",
    "input_layer = Input(shape=(PIXEL_HEIGHT, PIXEL_WIDTH, 3), dtype=tf.int8)\n",
    "x = NeuralNet(output_dim)(input_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# printing a view of the model\n",
    "print(model.summary(expand_nested=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 4s 498ms/step - loss: 2.6195 - acc: 0.3040 - val_loss: 1.7622 - val_acc: 0.4200\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 1.0565 - acc: 0.7580 - val_loss: 0.6536 - val_acc: 0.8900\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 4s 480ms/step - loss: 0.6570 - acc: 0.8300 - val_loss: 0.5408 - val_acc: 0.8400\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 4s 487ms/step - loss: 0.2332 - acc: 0.9660 - val_loss: 0.2316 - val_acc: 0.9700\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.1243 - acc: 0.9900 - val_loss: 0.1412 - val_acc: 0.9900\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 4s 458ms/step - loss: 0.0770 - acc: 1.0000 - val_loss: 0.1687 - val_acc: 0.9400\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.0660 - acc: 0.9960 - val_loss: 0.1046 - val_acc: 0.9900\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 4s 476ms/step - loss: 0.0426 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 4s 463ms/step - loss: 0.0410 - acc: 1.0000 - val_loss: 0.1049 - val_acc: 0.9800\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 4s 487ms/step - loss: 0.0295 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9900\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 4s 486ms/step - loss: 0.0250 - acc: 1.0000 - val_loss: 0.0869 - val_acc: 0.9700\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 4s 483ms/step - loss: 0.0236 - acc: 1.0000 - val_loss: 0.0661 - val_acc: 0.9900\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 4s 475ms/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 4s 517ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9800\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 4s 512ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.0622 - val_acc: 0.9800\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 4s 523ms/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 4s 517ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 4s 503ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 0.9900\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 4s 515ms/step - loss: 0.0138 - acc: 0.9980 - val_loss: 0.0499 - val_acc: 0.9900\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 4s 504ms/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# training the model now\n",
    "# model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, weight_decay=weight_decay), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=\"acc\")\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, weight_decay=weight_decay), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"acc\")\n",
    "history = model.fit(x=train_set, y=train_set_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(dev_set, dev_set_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 19:07:51.449670: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,34,45,20]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.460955: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,5000]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.469709: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,1000]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.695422: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,34,45,20]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.707311: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,5000]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.720008: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,1000]\n",
      "\t [[{{node inputs}}]]\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, rescaling_2_layer_call_fn, rescaling_2_layer_call_and_return_conditional_losses, conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses while saving (showing 5 of 23). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt3begs8z/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt3begs8z/assets\n",
      "2023-05-01 19:07:54.471642: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-05-01 19:07:54.471666: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-05-01 19:07:54.471804: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpt3begs8z\n",
      "2023-05-01 19:07:54.472611: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-01 19:07:54.472620: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpt3begs8z\n",
      "2023-05-01 19:07:54.475026: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-05-01 19:07:54.630340: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpt3begs8z\n",
      "2023-05-01 19:07:54.638716: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 166913 microseconds.\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('my_model')\n",
    "\n",
    "# Convert the model to the TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open('my_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_input_10:0', 'index': 0, 'shape': array([  1, 300, 400,   3], dtype=int32), 'shape_signature': array([ -1, 300, 400,   3], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 43, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Top 3 prections by (value, label)\n",
      "[(0.9983063, 4), (0.0005980613, 1), (0.00042299737, 5)]\n",
      "Actual label was: 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'my_model.tflite'\n",
    "interpreter = Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(input_details)\n",
    "print(output_details)\n",
    "single_test = np.reshape(dev_set[0], (1, 300, 400, 3))\n",
    "interpreter.set_tensor(input_details[0]['index'], single_test)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "print(\"Top 3 prections by (value, label)\")\n",
    "print(sorted(zip(output_data, [i for i in range(10)]), reverse=True)[:3])\n",
    "print(\"Actual label was: {}\\n\".format(dev_set_labels[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
