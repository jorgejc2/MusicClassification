{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to difficulties converting a Pytorch model to a Tensorflow model, this notebook uses the same CNN model as before, but written in Tensorflow. This allows it to be portable to an Android device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "from math import ceil, isnan\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import os.path as path\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "from PIL import Image\n",
    "\n",
    "import build.pybind_modules.dsp_module as cu\n",
    "import build.pybind_modules.matrix_module as myMatrix\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.math import confusion_matrix\n",
    "\n",
    "from tflite_runtime.interpreter import Interpreter\n",
    "\n",
    "print('TensorFlow version:',tf.__version__)\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "for dev in physical_devices:\n",
    "    print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "MODEL_NAME = 'audio_mnist'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Parameters used on tablet \n",
    "VOICED_THRESHOLD = 20000000\n",
    "FRAME_SETBACK = 2\n",
    "FS = 48000\n",
    "DOWN_SAMPLED_FS = 8000\n",
    "NFFT = 256\n",
    "NOVERLAP = -1\n",
    "NFILT = 40\n",
    "NUM_CEPS = 13\n",
    "NN_DATA_COLS = 28\n",
    "NN_DATA_ROWS = 12\n",
    "PREEMPHASIS_B = 0.97\n",
    "PIXEL_WIDTH = 400\n",
    "PIXEL_HEIGHT = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" helper functions used in sample processing before feed the samples to the CNN \"\"\"\n",
    "def soundDataToFloat(SD):\n",
    "    \"Converts integer representation back into librosa-friendly floats, given a numpy array SD\"\n",
    "    return np.array([ np.float32(s/32768.0) for s in SD])\n",
    "\n",
    "def soundDataToInt16(SD):\n",
    "    return np.array( [np.int16(s*32768.0) for s in SD] )\n",
    "\n",
    "def createButter(N, Wn, fs):\n",
    "    num, den = signal.butter(N, Wn, btype='low', analog=False, output='ba', fs=fs)\n",
    "    return np.array([num[i]/den[i] for i in range(len(num))])\n",
    "\n",
    "def createFIR(num_taps, cut_off, fs):\n",
    "    return signal.firwin(num_taps, cut_off, fs=fs)\n",
    "\n",
    "def displayFIR(filt):\n",
    "    coef_str = \"{\" \n",
    "    for val in filt: \n",
    "        coef_str += str(val) + \", \" \n",
    "    coef_str = coef_str[:-2] \n",
    "    coef_str += \"};\" \n",
    "    print(\"FIR a Coefficients\")\n",
    "    print(coef_str) \n",
    "\n",
    "def applyFIR(samples, filt):\n",
    "    circBuf = np.zeros(len(filt))\n",
    "    circBufIdx = 0\n",
    "    filteredSamples = np.zeros(len(samples))\n",
    "    num_taps = len(filt)\n",
    "\n",
    "    for i in range(len(samples)):\n",
    "        circBuf[circBufIdx] = samples[i]\n",
    "        curr_val = 0\n",
    "\n",
    "        for n in range(num_taps):\n",
    "            curr_val += filt[n] * circBuf[ (((circBufIdx - n) % num_taps) + num_taps) % num_taps]\n",
    "\n",
    "        filteredSamples[i] = curr_val \n",
    "        circBufIdx = (circBufIdx + 1) % num_taps\n",
    "\n",
    "    return filteredSamples\n",
    "\n",
    "def frameVoiced(frame, threshold):\n",
    "    isVoiced = False\n",
    "    sum = 0\n",
    "    for i in range(len(frame)):\n",
    "        sum += abs(frame[i])**2\n",
    "\n",
    "    if (sum > threshold):\n",
    "        isVoiced = 1\n",
    "    \n",
    "    return isVoiced\n",
    "\n",
    "def trimSamples(samples, frameSize, nfft, noverlap, threshold, frame_setback):\n",
    "    num_samples = len(samples)\n",
    "    if (noverlap < 0):\n",
    "        noverlap = int(nfft/2)\n",
    "\n",
    "    step = nfft - noverlap\n",
    "\n",
    "    numFrames = ceil(num_samples / step)\n",
    "\n",
    "    while ((numFrames - 1)*step + (nfft - 1) >= num_samples):\n",
    "        numFrames -= 1\n",
    "\n",
    "    first_frame = 0\n",
    "    for i in range(numFrames):\n",
    "        if frameVoiced(soundDataToInt16(samples[i*step:i*step + nfft]), threshold=threshold):\n",
    "            first_frame = i\n",
    "            break\n",
    "    \n",
    "    first_frame -= frame_setback\n",
    "    if first_frame < 0:\n",
    "        first_frame = 0\n",
    "    last_frame = first_frame + frameSize\n",
    "    num_trimmed_samples = ((last_frame - 1)*step + nfft) - (first_frame*step)\n",
    "    trimmed_samples = np.zeros(num_trimmed_samples)\n",
    "\n",
    "    for i in range(num_trimmed_samples):\n",
    "        if (first_frame*step + i >= len(samples)):\n",
    "            break\n",
    "        trimmed_samples[i] = samples[first_frame*step + i]\n",
    "\n",
    "    return trimmed_samples\n",
    "\n",
    "def createImage(data, pixel_width, pixel_height, data_rows, data_cols, filename=None):\n",
    "    def hex_to_rgb(val):\n",
    "        mask = 0x0000FF\n",
    "        b = mask & val\n",
    "        g = mask & (val >> 8)\n",
    "        r = mask & (val >> 16)\n",
    "        \n",
    "        return (r,g,b)\n",
    "\n",
    "    viridis_pallete = [\n",
    "        0x440154,\n",
    "        0x481567,\n",
    "        0x482677,\n",
    "        0x453771,\n",
    "        0x404788,\n",
    "        0x39568C,\n",
    "        0x33638D,\n",
    "        0x2D708E,\n",
    "        0x287D8E,\n",
    "        0x238A8D,\n",
    "        0x1F968B,\n",
    "        0x20A387,\n",
    "        0x29AF7F,\n",
    "        0x3CBB75,\n",
    "        0x55C667,\n",
    "        0x73D055,\n",
    "        0x95D840,\n",
    "        0xB8DE29,\n",
    "        0xDCE319,\n",
    "        0xFDE725\n",
    "    ]\n",
    "    viridis_pallete_rgb = [hex_to_rgb(x) for x in viridis_pallete]\n",
    "    viridis_size = len(viridis_pallete_rgb)\n",
    "\n",
    "    max_val = None\n",
    "    min_val = None\n",
    "    for y in range(data_rows):\n",
    "        for x in range(data_cols):\n",
    "            sample = data[y,x]\n",
    "            if isnan(sample):\n",
    "                sample = 0\n",
    "                data[y,x] = sample\n",
    "            if max_val == None or sample > max_val:\n",
    "                max_val = sample\n",
    "            if min_val == None or sample < min_val:\n",
    "                min_val = sample\n",
    "\n",
    "    max_val -= min_val\n",
    "    data = (data-min_val) / max_val\n",
    "\n",
    "    \"\"\" canvas for holding rgb image from the range 0 to 255 \"\"\"\n",
    "    canvas = np.zeros((pixel_height, pixel_width, 3), dtype=np.uint8)\n",
    "    horizontal_step = int(pixel_width / data_cols)\n",
    "    vertical_step = int(pixel_height / data_rows)\n",
    "\n",
    "    # attempting to only manipulate green in rgb\n",
    "\n",
    "    # step counters that are 1 indexed\n",
    "    horizontal_count = 1\n",
    "    vertical_count = 1\n",
    "    for pixel_row in range(pixel_height):\n",
    "        if (pixel_row >= vertical_count * vertical_step) and (vertical_count < data_rows):\n",
    "            vertical_count += 1\n",
    "\n",
    "        horizontal_count = 1\n",
    "        for pixel_col in range(pixel_width):\n",
    "            if (pixel_col >= horizontal_count * horizontal_step) and (horizontal_count < data_cols):\n",
    "                horizontal_count += 1\n",
    "            \n",
    "            # 0 index\n",
    "            x_idx = horizontal_count - 1\n",
    "            y_idx = data_rows - vertical_count\n",
    "\n",
    "            # determine green value from data\n",
    "            percent = data[y_idx, x_idx]\n",
    "\n",
    "            viridis_idx = int((viridis_size-1) * percent)\n",
    "            curr_color = viridis_pallete_rgb[viridis_idx]\n",
    "\n",
    "            red_val = curr_color[0]\n",
    "            green_val = curr_color[1]\n",
    "            blue_val = curr_color[2]\n",
    "            \n",
    "            canvas[pixel_row, pixel_col, 0] = red_val\n",
    "            canvas[pixel_row, pixel_col, 1] = green_val\n",
    "            canvas[pixel_row, pixel_col, 2] = blue_val\n",
    "\n",
    "    im = Image.fromarray(canvas)\n",
    "    if filename is not None:\n",
    "        im.save(filename) # if you wanted to save the image\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu6(x):\n",
    "    return tf.keras.activations.relu(x, max_value=6)\n",
    "\n",
    "\n",
    "class NeuralNet(tf.keras.Model):\n",
    "    def __init__(self, out_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.rescaling = tf.keras.layers.Rescaling(1./255, input_shape=(300, 400, 3))\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3), activation=relu6, padding='same', kernel_initializer='he_uniform')\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(3,3), padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=20, kernel_size=(3,3), activation=relu6, padding='same', kernel_initializer='he_uniform')\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate=0.1)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate=0.16)\n",
    "        self.dropout_3 = tf.keras.layers.Dropout(rate=0.12)\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense_1 = tf.keras.layers.Dense(units=5000, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.dense_2 = tf.keras.layers.Dense(units=1000, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.dense_3 = tf.keras.layers.Dense(units=out_size, activation='softmax', kernel_initializer='he_uniform')\n",
    "\n",
    "        # self.loss_fn = loss_fn\n",
    "        # self.optimizer = tf.keras.optimizers.SGD(learning_rate=lrate, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.rescaling(x)\n",
    "        # print(\"Done rescaling\")\n",
    "        x = self.conv1(x)\n",
    "        # print(\"Done conv1\")\n",
    "        x = self.maxpool(x)\n",
    "        # print(\"Done maxpool1\")\n",
    "        x = self.conv2(x)\n",
    "        # print(\"Done conv2\")\n",
    "        x = self.maxpool(x)\n",
    "        # print(\"Done maxpool2\")\n",
    "        x = self.dropout_1(x, training=True)\n",
    "        # print(\"Done droput1\")\n",
    "        x = self.flatten(x)\n",
    "        # print(\"Done flatten\")\n",
    "        x = self.dense_1(x)\n",
    "        # print(\"Done dense 1\")\n",
    "        x = self.dropout_2(x, training=True)\n",
    "        # print(\"Done dropout 2\")\n",
    "        x = self.dense_2(x)\n",
    "        # print(\"Done dense 3\")\n",
    "        x = self.dropout_3(x, training=True)\n",
    "        # print(\"Done dropout 3\")\n",
    "        x = self.dense_3(x)\n",
    "        # print(\"Done dense 3\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 400, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=400x300 at 0x7F6A37BF4EE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x33\n",
      "0x63\n",
      "0x8d\n",
      "0xff\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=400x300 at 0x7F6A7CAEE4C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the data set\n",
    "\n",
    "path = \"MFCC_Images/0/mfcc_images/Mon May 01 14:35:23 CDT 2023_mfcc.png\"\n",
    "test_image = Image.open(path)\n",
    "image_data = np.array(test_image)\n",
    "print(image_data.shape)\n",
    "display(test_image)\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"{}\".format(hex(image_data[10][20][i])))\n",
    "\n",
    "\"\"\" \n",
    "Noticed that the fourth channel is alpha which is always xFF.\n",
    "Now I will try to figure out the RGB order by mutating the image.\n",
    "\"\"\"\n",
    "for i in range(image_data.shape[0]):\n",
    "    for j in range(image_data.shape[1]):\n",
    "        pixel = image_data[i,j]\n",
    "        pixel[2] = 0\n",
    "\"\"\"\n",
    "From experimentation:\n",
    "    channel 0 -- r\n",
    "    channel 1 -- g\n",
    "    channel 2 -- b\n",
    "    channel 3 -- alpha (always 0xFF) (also must be the most signicant byte)\n",
    "\n",
    "This is good because it verifies that the breakdown is the same as the images on the Android tablet. \n",
    "\"\"\"\n",
    "# now figure out which channel is green\n",
    "mutated_image = Image.fromarray(image_data)\n",
    "display(mutated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shuffle shape: (500, 300, 400, 4)\n",
      "after shuffle shape: (500, 300, 400, 4)\n",
      "len train set: 500\n",
      "len train label set: 500\n",
      "len dev set: 100\n",
      "len dev label set: 100\n",
      "[3. 5. 7. 0. 5. 2. 7. 2. 0. 1. 8. 6. 9. 9. 8. 3. 3. 8. 4. 5. 9. 9. 8. 7.\n",
      " 5. 0. 7. 8. 0. 5. 6. 5. 0. 3. 4. 2. 8. 5. 8. 4. 3. 1. 9. 9. 7. 5. 3. 4.\n",
      " 9. 6. 8. 2. 7. 5. 3. 9. 0. 5. 1. 7. 9. 4. 3. 4. 6. 1. 8. 0. 0. 5. 9. 7.\n",
      " 0. 5. 4. 6. 4. 8. 1. 2. 2. 7. 6. 4. 3. 0. 1. 3. 8. 6. 4. 3. 0. 8. 9. 3.\n",
      " 4. 8. 9. 6. 2. 6. 9. 5. 4. 0. 0. 6. 1. 5. 1. 2. 2. 3. 3. 9. 3. 4. 2. 7.\n",
      " 0. 4. 7. 7. 1. 5. 4. 2. 7. 4. 7. 5. 8. 1. 9. 6. 6. 1. 0. 9. 0. 8. 3. 7.\n",
      " 0. 6. 2. 0. 9. 8. 5. 7. 2. 8. 2. 0. 5. 7. 8. 0. 0. 0. 5. 1. 1. 8. 2. 1.\n",
      " 9. 1. 6. 2. 7. 7. 7. 9. 7. 1. 9. 6. 6. 2. 4. 0. 6. 6. 8. 6. 6. 8. 1. 3.\n",
      " 5. 6. 9. 0. 0. 4. 7. 2. 8. 5. 9. 7. 5. 3. 2. 8. 6. 3. 2. 2. 1. 9. 3. 3.\n",
      " 1. 0. 1. 8. 1. 4. 1. 3. 4. 3. 1. 2. 9. 7. 7. 4. 8. 7. 6. 2. 1. 5. 8. 7.\n",
      " 8. 7. 4. 0. 6. 3. 4. 1. 5. 4. 4. 1. 8. 3. 7. 9. 5. 1. 3. 2. 6. 5. 2. 6.\n",
      " 1. 8. 2. 4. 2. 0. 5. 0. 7. 3. 2. 9. 6. 5. 1. 3. 3. 8. 7. 7. 6. 4. 5. 6.\n",
      " 8. 7. 2. 5. 6. 5. 8. 6. 5. 9. 9. 2. 8. 6. 0. 9. 3. 3. 3. 4. 0. 0. 2. 5.\n",
      " 9. 6. 7. 2. 6. 7. 7. 5. 0. 3. 4. 7. 9. 8. 1. 4. 9. 6. 0. 1. 3. 9. 1. 8.\n",
      " 2. 2. 5. 3. 5. 2. 5. 6. 5. 0. 3. 2. 8. 1. 9. 1. 3. 5. 4. 5. 8. 9. 6. 4.\n",
      " 9. 9. 4. 3. 4. 2. 5. 6. 8. 4. 2. 3. 0. 7. 3. 3. 6. 8. 1. 5. 4. 2. 1. 1.\n",
      " 0. 1. 7. 3. 2. 7. 8. 1. 0. 0. 7. 6. 6. 9. 5. 1. 6. 3. 9. 4. 4. 0. 1. 4.\n",
      " 7. 8. 9. 4. 7. 4. 7. 9. 8. 0. 1. 2. 5. 8. 4. 0. 4. 2. 6. 2. 5. 3. 4. 8.\n",
      " 1. 6. 4. 3. 9. 4. 3. 8. 5. 8. 9. 6. 3. 2. 0. 1. 8. 2. 7. 1. 9. 7. 8. 2.\n",
      " 0. 5. 8. 0. 0. 1. 2. 4. 9. 7. 1. 2. 0. 6. 8. 0. 2. 6. 7. 4. 1. 1. 7. 9.\n",
      " 0. 5. 4. 2. 0. 1. 9. 6. 9. 7. 5. 4. 6. 5. 6. 9. 3. 9. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "train_files_per_label = 50\n",
    "dev_files_per_label = 60 - train_files_per_label\n",
    "dev_set = np.zeros((dev_files_per_label*10, 300, 400, 3), dtype=np.int8)\n",
    "dev_set_labels = np.zeros(dev_files_per_label * 10)\n",
    "raw_train_set = np.zeros((train_files_per_label*10, 300, 400, 4), dtype=np.int8)\n",
    "train_set_labels = np.zeros(train_files_per_label * 10)\n",
    "\n",
    "path = os.getcwd() + \"/MFCC_Images\"\n",
    "labeled_directories = os.listdir(path)\n",
    "\n",
    "for i in range(len(labeled_directories)):\n",
    "    label = labeled_directories[i]\n",
    "    curr_label = int(label)\n",
    "    # print(curr_label)\n",
    "\n",
    "    image_directory_path = f\"{path}/{label}/mfcc_images\"\n",
    "    image_directory = os.listdir(image_directory_path)\n",
    "    \n",
    "    for j in range(len(image_directory)):\n",
    "        image = image_directory[j]\n",
    "        image_path = f\"{image_directory_path}/{image}\"\n",
    "        curr_image = Image.open(image_path)\n",
    "        if j < train_files_per_label:\n",
    "            raw_train_set[(i*train_files_per_label) + j,:,:,:3] = np.array(curr_image)[:,:,:3]\n",
    "            raw_train_set[(i*train_files_per_label) + j,:,:,3] = curr_label\n",
    "        else:\n",
    "            dev_set[(i*dev_files_per_label) + (j - train_files_per_label)] = np.array(curr_image)[:,:,:3]\n",
    "            dev_set_labels[(i*dev_files_per_label) + (j - train_files_per_label)] = curr_label\n",
    "\n",
    "# now randomize the training set\n",
    "# note that this only shuffles along axis=0 which is what we want\n",
    "print(\"before shuffle shape: {}\".format(raw_train_set.shape))\n",
    "np.random.shuffle(raw_train_set)\n",
    "print(\"after shuffle shape: {}\".format(raw_train_set.shape))\n",
    "train_set_label_idx = 0\n",
    "for i in range(raw_train_set.shape[0]):\n",
    "    train_set_labels[train_set_label_idx] = raw_train_set[i,0,0,3]\n",
    "    train_set_label_idx += 1\n",
    "train_set = np.zeros((train_files_per_label*10, 300, 400, 3), dtype=np.int8)\n",
    "train_set[:,:,:,:] = raw_train_set[:,:,:,:3]\n",
    "\n",
    "print(\"len train set: {}\".format(len(train_set)))\n",
    "print(\"len train label set: {}\".format(len(train_set_labels)))\n",
    "print(\"len dev set: {}\".format(len(dev_set)))\n",
    "print(\"len dev label set: {}\".format(len(dev_set_labels)))\n",
    "\n",
    "print(train_set_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below will not be finished. It was meant to incorporate the AudioMNIST dataset into our own personalized dataset, but after running a few voice files from the dataset with our parameters before the MFCC algorithm, it is clear that the dataset may ruin the integrity of the neural network. The biggest concern is the variation in the duration of a number that is said between different speakers. Numbers such as 7 are spoken over a longer duration than our algorithm can capture. To better incorporate this, a useful tool would be to create a more sophistaced pitch detection algorithm that can capture the the beginning and ending of an utterance of a spoken digit. It may also be worth exploring long short term memory recurrent neural networks which can take multiple time frames. It could also be worth exploring other classification techniques such as guassian mixture models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" This block will incorporate the AudioMNIST data set as well. Do not run this if you only want to train on the specific speaker from above \"\"\"\n",
    "\n",
    "# path = \"AudioMNIST/data/53/7_53_0.wav\"\n",
    "# fs, samples = wavfile.read(path)\n",
    "\n",
    "# sig = soundDataToFloat(samples)\n",
    "\n",
    "# filt = createFIR(51, 3500, fs=fs)\n",
    "# filtered_sig = applyFIR(sig, filt)\n",
    "\n",
    "# filtered_sig = np.array([filtered_sig[i] for i in range(0, len(filtered_sig), 6)])\n",
    "\n",
    "# filtered_sig = trimSamples(filtered_sig, NN_DATA_COLS, NFFT, NOVERLAP, VOICED_THRESHOLD, FRAME_SETBACK)\n",
    "\n",
    "# Audio(filtered_sig, rate=DOWN_SAMPLED_FS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 300, 400, 3)]     0         \n",
      "                                                                 \n",
      " neural_net_9 (NeuralNet)    (None, 10)                158018110 \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rescaling_2 (Rescaling)   multiple                  0         |\n",
      "|                                                               |\n",
      "| conv2d_18 (Conv2D)        multiple                  280       |\n",
      "|                                                               |\n",
      "| max_pooling2d_9 (MaxPooling  multiple               0         |\n",
      "| 2D)                                                           |\n",
      "|                                                               |\n",
      "| conv2d_19 (Conv2D)        multiple                  1820      |\n",
      "|                                                               |\n",
      "| dropout_27 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| dropout_28 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| dropout_29 (Dropout)      multiple                  0         |\n",
      "|                                                               |\n",
      "| flatten_9 (Flatten)       multiple                  0         |\n",
      "|                                                               |\n",
      "| dense_27 (Dense)          multiple                  153005000 |\n",
      "|                                                               |\n",
      "| dense_28 (Dense)          multiple                  5001000   |\n",
      "|                                                               |\n",
      "| dense_29 (Dense)          multiple                  10010     |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "=================================================================\n",
      "Total params: 158,018,110\n",
      "Trainable params: 158,018,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# running dataset on model \n",
    "input_dim = 12 # num ceps - 1\n",
    "output_dim = 10 # number of genres\n",
    "weight_decay = 1e-2\n",
    "learning_rate = 1e-2\n",
    "\n",
    "input_layer = Input(shape=(PIXEL_HEIGHT, PIXEL_WIDTH, 3), dtype=tf.int8)\n",
    "x = NeuralNet(output_dim)(input_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# printing a view of the model\n",
    "print(model.summary(expand_nested=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 4s 498ms/step - loss: 2.6195 - acc: 0.3040 - val_loss: 1.7622 - val_acc: 0.4200\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 1.0565 - acc: 0.7580 - val_loss: 0.6536 - val_acc: 0.8900\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 4s 480ms/step - loss: 0.6570 - acc: 0.8300 - val_loss: 0.5408 - val_acc: 0.8400\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 4s 487ms/step - loss: 0.2332 - acc: 0.9660 - val_loss: 0.2316 - val_acc: 0.9700\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.1243 - acc: 0.9900 - val_loss: 0.1412 - val_acc: 0.9900\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 4s 458ms/step - loss: 0.0770 - acc: 1.0000 - val_loss: 0.1687 - val_acc: 0.9400\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 4s 467ms/step - loss: 0.0660 - acc: 0.9960 - val_loss: 0.1046 - val_acc: 0.9900\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 4s 476ms/step - loss: 0.0426 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 4s 463ms/step - loss: 0.0410 - acc: 1.0000 - val_loss: 0.1049 - val_acc: 0.9800\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 4s 487ms/step - loss: 0.0295 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9900\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 4s 486ms/step - loss: 0.0250 - acc: 1.0000 - val_loss: 0.0869 - val_acc: 0.9700\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 4s 483ms/step - loss: 0.0236 - acc: 1.0000 - val_loss: 0.0661 - val_acc: 0.9900\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 4s 475ms/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 4s 517ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9800\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 4s 512ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.0622 - val_acc: 0.9800\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 4s 523ms/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 4s 517ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 4s 503ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 0.9900\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 4s 515ms/step - loss: 0.0138 - acc: 0.9980 - val_loss: 0.0499 - val_acc: 0.9900\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 4s 504ms/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# training the model now\n",
    "# model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, weight_decay=weight_decay), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=\"acc\")\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, weight_decay=weight_decay), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"acc\")\n",
    "history = model.fit(x=train_set, y=train_set_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(dev_set, dev_set_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 19:07:51.449670: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,34,45,20]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.460955: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,5000]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.469709: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,1000]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.695422: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,34,45,20]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.707311: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,5000]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-01 19:07:51.720008: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,1000]\n",
      "\t [[{{node inputs}}]]\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, rescaling_2_layer_call_fn, rescaling_2_layer_call_and_return_conditional_losses, conv2d_18_layer_call_fn, conv2d_18_layer_call_and_return_conditional_losses while saving (showing 5 of 23). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt3begs8z/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt3begs8z/assets\n",
      "2023-05-01 19:07:54.471642: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-05-01 19:07:54.471666: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-05-01 19:07:54.471804: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpt3begs8z\n",
      "2023-05-01 19:07:54.472611: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-01 19:07:54.472620: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpt3begs8z\n",
      "2023-05-01 19:07:54.475026: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-05-01 19:07:54.630340: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpt3begs8z\n",
      "2023-05-01 19:07:54.638716: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 166913 microseconds.\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('my_model')\n",
    "\n",
    "# Convert the model to the TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open('my_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_input_10:0', 'index': 0, 'shape': array([  1, 300, 400,   3], dtype=int32), 'shape_signature': array([ -1, 300, 400,   3], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 43, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Top 3 prections by (value, label)\n",
      "[(0.9983063, 4), (0.0005980613, 1), (0.00042299737, 5)]\n",
      "Actual label was: 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'my_model.tflite'\n",
    "interpreter = Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(input_details)\n",
    "print(output_details)\n",
    "single_test = np.reshape(dev_set[0], (1, 300, 400, 3))\n",
    "interpreter.set_tensor(input_details[0]['index'], single_test)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "print(\"Top 3 prections by (value, label)\")\n",
    "print(sorted(zip(output_data, [i for i in range(10)]), reverse=True)[:3])\n",
    "print(\"Actual label was: {}\\n\".format(dev_set_labels[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
